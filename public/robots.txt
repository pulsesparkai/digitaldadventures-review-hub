# Robots.txt for DigitalDadVentures
# https://digitaldadventures.com/robots.txt

# Google Crawlers
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Googlebot-Image
Allow: /
Crawl-delay: 1

User-agent: Googlebot-News
Allow: /
Crawl-delay: 1

User-agent: Googlebot-Video
Allow: /
Crawl-delay: 1

# Bing Crawler
User-agent: Bingbot
Allow: /
Crawl-delay: 2

# Social Media Crawlers
User-agent: Twitterbot
Allow: /

User-agent: facebookexternalhit
Allow: /

User-agent: LinkedInBot
Allow: /

# Other Search Engines
User-agent: DuckDuckBot
Allow: /
Crawl-delay: 1

User-agent: Slurp
Allow: /
Crawl-delay: 2

# Generic Rules for All Bots
User-agent: *
Allow: /

# Disallow admin and sensitive areas
Disallow: /admin/
Disallow: /api/
Disallow: /private/
Disallow: /.git/
Disallow: /node_modules/

# Disallow search and filter parameters
Disallow: /*?search=*
Disallow: /*?filter=*
Disallow: /*?sort=*
Disallow: /*?page=*

# Disallow UTM and tracking parameters
Disallow: /*?utm_*
Disallow: /*?ref=*
Disallow: /*?source=*
Disallow: /*?medium=*
Disallow: /*?campaign=*

# Disallow duplicate content with trailing slashes
Disallow: /*/

# Allow important directories
Allow: /images/
Allow: /css/
Allow: /js/
Allow: /assets/

# Sitemap location
Sitemap: https://digitaldadventures.com/sitemap.xml